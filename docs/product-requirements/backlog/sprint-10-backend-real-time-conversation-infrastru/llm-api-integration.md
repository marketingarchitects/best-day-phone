---
wrike_id: MAAAAAEDAoDp
title: "LLM API Integration"
status: Active
importance: Normal
permalink: https://www.wrike.com/open.htm?id=4345463017
updated_date: 2026-01-08T22:09:27Z
last_sync: 2026-01-09T14:04:48.395522
authors:
  - "Noah Moss"
---

# LLM API Integration

**Status:** ðŸŸ¢ Active
**Wrike:** [Open in Wrike](https://www.wrike.com/open.htm?id=4345463017)

---

## Description

As a system, I need LLM API integration so that AI conversations are generated.Conversation Memory- Â Short-Term Memory (Within Call)Maintain conversation history inÂ messagesÂ array
- Keep last 20 turns (patient &#43; AI) in context
- Prune older turns to stay within token limits
- Â Long-Term Memory (Across Calls)GivenÂ conversation ends,Â WhenÂ processing,Â Then:Generate summary of conversation 
- Store key facts mentioned by patient inÂ patient_memories:&#34;Patient mentioned granddaughter Emma started 3rd grade&#34;
- &#34;Patient's tomatoes are growing well&#34;
- Load relevant memories in next conversation context
Guardrails & Safety- Â Content ModerationGivenÂ patient says concerning content,Â WhenÂ detected,Â Then:Flag in conversation log for caregiver review
- AI responds compassionately but doesn't escalate
- Examples: Expressions of sadness, loneliness (normal), vs. self-harm (requires flag)
- Â Topic Avoidance EnforcementGivenÂ patient mentions avoided topic,Â WhenÂ detected,Â Then:AI acknowledges briefly
- Gently redirects: &#34;I understand. Tell me about {positive_interest}?&#34;
- Log topic mention for caregiver
- Â Medical Advice PreventionGivenÂ patient asks medical question,Â WhenÂ detected,Â Then:AI responds: &#34;I'm not able to provide medical advice. It's best to speak with your doctor about that. How about we talk about {other_topic}?&#34;
Cost Optimization- Â Token ManagementMonitor tokens per conversation
- Store inÂ conversations.llm_tokens_usedÂ (input &#43; output)
- Alert if conversation exceeds 10,000 tokens (unusually long)
- Â Model SelectionA/B test different models for quality difference
Testing Requirements- Â Integration test: LLM generates coherent responses
- Â Integration test: Context includes all patient profile data
- Â Unit test: Avoided topics trigger redirection
- Â Unit test: Medical advice requests handled correctly
- Â Performance test: LLM response latency < 800ms (p95)
